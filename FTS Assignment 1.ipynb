{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short-term reversal alpha factor\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The concept of market efficiency is frequently being debated by academics and those in the finance industry. On one hand, many believe in the Efficient Market Hypothesis. The EMH, suggests that stock prices fully reflect all available information about a firm’s value making it impossible for someone to gain excess profits. This opinion is supported by research conducted by Eugene F. Fama in the 1960s (1). Under this hypothesis, short term deviations in prices are expected to be random and unpredictable (2). \n",
    "\n",
    "On the other hand, an extensive range of academics have provided evidence that anomalies exist in financial markets which somewhat explain deviation in stock prices (3). Stylized facts are a term used to refer to “empirical findings that are so consistent across markets that they are accepted as truth” (4). ‘Dependence’ is a stylized fact category which looks at autocorrelation (both positive and negative) in stock returns. Short-term reversal is one of the most documented financial anomalies in asset pricing literature which capitalizes on predicting negative autocorrelation in stock returns. It is the theory that last week’s ‘winning’ shares will see abnormal negative returns for the following week, while last week’s ‘losing’ shares will see abnormal positive returns. \n",
    "\n",
    "This Research Notebook aims to provide an understanding of the short-term reversal phenomenon and will analyse whether it exists in international equity markets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantopian.pipeline import Pipeline, CustomFactor\n",
    "from quantopian.research import run_pipeline\n",
    "\n",
    "from quantopian.pipeline.data import USEquityPricing, EquityPricing, factset\n",
    "from quantopian.pipeline.filters import QTradableStocksUS\n",
    "from quantopian.pipeline.classifiers.fundamentals import Sector\n",
    "\n",
    "from alphalens.utils import get_clean_factor_and_forward_returns\n",
    "from alphalens.tears import create_full_tear_sheet, create_information_tear_sheet, create_returns_tear_sheet\n",
    "from alphalens.performance import mean_information_coefficient\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveDayReversalUS(CustomFactor):\n",
    "    \n",
    "    inputs = [USEquityPricing.close]\n",
    "    window_length = 5 \n",
    "# the current price minus the price 5 days ago, divided by the price 5 days ago \n",
    "    def compute(self,today,assets,out,close):\n",
    "        out[:] = -(close[self.window_length-1] - close[0])/close[0]\n",
    "        \n",
    "reversal_factor5 = FiveDayReversalUS()\n",
    "\n",
    "class TenDayReversalUS(CustomFactor):\n",
    "    \n",
    "    inputs = [USEquityPricing.close]\n",
    "    window_length = 10 \n",
    " \n",
    "    def compute(self,today,assets,out,close):\n",
    "        out[:] = -(close[self.window_length-1] - close[0])/close[0]\n",
    "        \n",
    "reversal_factor10 = TenDayReversalUS()\n",
    "\n",
    "class MonthlyReversalUS(CustomFactor):\n",
    "    \n",
    "    inputs = [USEquityPricing.close]\n",
    "    window_length = 21 \n",
    " \n",
    "    def compute(self,today,assets,out,close):\n",
    "        out[:] = -(close[self.window_length-1] - close[0])/close[0]\n",
    "        \n",
    "reversal_factor21 = MonthlyReversalUS()\n",
    "\n",
    "combined_factor = reversal_factor5 + reversal_factor10 + reversal_factor21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline():\n",
    "    return Pipeline(\n",
    "        \n",
    "        columns = {\n",
    "            'weekly_reversal': reversal_factor5,\n",
    "            'fortnitely_reversal': reversal_factor10,\n",
    "            'monthly_reversal': reversal_factor21,\n",
    "            'combined_factor': combined_factor,\n",
    "        },\n",
    "        \n",
    "        screen = (\n",
    "            QTradableStocksUS() \n",
    "            & reversal_factor5.notnull() \n",
    "            & reversal_factor10.notnull()\n",
    "            & reversal_factor21.notnull()\n",
    "            & combined_factor.notnull()\n",
    "        )\n",
    "    )\n",
    "\n",
    "factor_data = run_pipeline(pipeline = make_pipeline(),start_date='2010-01-01', end_date='2016-01-01')\n",
    "\n",
    "factor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing_data = get_pricing(symbols=factor_data.index.levels[1],\n",
    "                          start_date='2010-01-01', end_date='2016-03-01', fields='open_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the time periods over which we will calculate the IC mean figures.\n",
    "longest_look_forward_period = 40 # Common time periods: week = 5, month = 21, quarter = 63, year = 252\n",
    "range_step = 5 # A larger number here makes things run faster, but makes the resulting chart less precise\n",
    "\n",
    "# We will populate this empty dataframe with IC Mean data\n",
    "factor_ic_decay = pd.DataFrame()\n",
    "\n",
    "for factor_name in factor_data.columns:\n",
    "    \n",
    "    # Excludes the 'sector' column in pipeline_output\n",
    "    if factor_name != 'sector':\n",
    "        \n",
    "        # Gets the IC decay for each factor\n",
    "        asset_factor_data = get_clean_factor_and_forward_returns(\n",
    "            factor = factor_data[factor_name],\n",
    "            prices = pricing_data,\n",
    "            periods = range(1, longest_look_forward_period, range_step)\n",
    "        )\n",
    "        factor_ic_decay[factor_name] = mean_information_coefficient(asset_factor_data)\n",
    "\n",
    "# Plots the dataframe that has been populated with IC decay information\n",
    "factor_ic_decay.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline():\n",
    "    return Pipeline(\n",
    "        \n",
    "        columns = {\n",
    "            'weekly_reversal': reversal_factor5,\n",
    "        },\n",
    "        \n",
    "        screen = (QTradableStocksUS() & reversal_factor5.notnull() \n",
    "        )\n",
    "    )\n",
    "\n",
    "factor1_data = run_pipeline(pipeline = make_pipeline(),start_date='2010-01-01', end_date='2016-01-01')\n",
    "\n",
    "factor1_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = get_clean_factor_and_forward_returns(\n",
    "    factor = factor1_data,\n",
    "    prices = pricing_data,\n",
    "    quantiles=10,\n",
    "    periods= [5]\n",
    "\n",
    "    )\n",
    "\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_full_tear_sheet(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Vol_3M(CustomFactor):\n",
    "\n",
    "        inputs = [USEquityPricing.close]\n",
    "        window_length = 63\n",
    "\n",
    "        def compute(self, today, assets, out, close):\n",
    "\n",
    "            vols = []\n",
    "            for col in close.T:\n",
    "                # compute returns\n",
    "                log_col_returns = np.log(col / np.roll(col, 1))[1:]\n",
    "                vols.append(np.nanstd(log_col_returns))\n",
    "            out[:] = vols\n",
    "            \n",
    "volatility_min = Vol_3M()\n",
    "\n",
    "volatility_filter = (volatility_min.percentile_between(50,100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumPeriodReturnsUS(CustomFactor):\n",
    "    \n",
    "    inputs = [USEquityPricing.close]\n",
    "    window_length = 250 \n",
    " \n",
    "    def compute(self,today,assets,out,close):\n",
    "        out[:] = (close[self.window_length-1] - close[0])/close[0]\n",
    "        \n",
    "momentum_max = MomentumPeriodReturnsUS()\n",
    "\n",
    "momentum_filter = (momentum_max.percentile_between(15,90,)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxVolumeUS(CustomFactor):\n",
    "    inputs=[USEquityPricing.volume]\n",
    "    window_length=5\n",
    "    mask = QTradableStocksUS()\n",
    "    def compute(self, today, asset_ids, out, values):\n",
    "        out[:] = np.min(values, axis=0)\n",
    "        \n",
    "# Create a volume filter that filters for stocks in the bottom 50% of our MinVolume factor.\n",
    "volume_max = MaxVolumeUS()\n",
    "volume_filter = (volume_max.percentile_between(0, 50, mask=(volume_max > 0)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline():\n",
    "    return Pipeline(\n",
    "        \n",
    "        columns = {\n",
    "            'weekly_reversal': reversal_factor5,\n",
    "        },\n",
    "        \n",
    "        screen = (\n",
    "            QTradableStocksUS() \n",
    "            & reversal_factor5.notnull()\n",
    "            & momentum_filter\n",
    "            & volume_filter\n",
    "        )\n",
    "    )\n",
    "\n",
    "factor2_data = run_pipeline(pipeline = make_pipeline(),start_date='2010-01-01', end_date='2016-01-01')\n",
    "\n",
    "factor2_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = get_clean_factor_and_forward_returns(\n",
    "    factor = factor2_data,\n",
    "    prices = pricing_data,\n",
    "    quantiles=10,\n",
    "    periods= [5]\n",
    "\n",
    "    )\n",
    "\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_full_tear_sheet(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantopian.pipeline.domain import (\n",
    "    AT_EQUITIES, # Austria\n",
    "    AU_EQUITIES, # Australia\n",
    "    BE_EQUITIES, # Belgium\n",
    "    BR_EQUITIES, # Brazil\n",
    "    CA_EQUITIES, # Canada\n",
    "    CH_EQUITIES, # Switzerland\n",
    "    CN_EQUITIES, # China\n",
    "    DE_EQUITIES, # Germany\n",
    "    DK_EQUITIES, # Denmark\n",
    "    ES_EQUITIES, # Spain\n",
    "    FI_EQUITIES, # Finland\n",
    "    FR_EQUITIES, # France\n",
    "    GB_EQUITIES, # Great Britain\n",
    "    HK_EQUITIES, # Hong Kong\n",
    "    IE_EQUITIES, # Ireland\n",
    "    IN_EQUITIES, # India\n",
    "    IT_EQUITIES, # Italy\n",
    "    JP_EQUITIES, # Japan\n",
    "    KR_EQUITIES, # South Korea\n",
    "    NL_EQUITIES, # Netherlands\n",
    "    NO_EQUITIES, # Norway\n",
    "    NZ_EQUITIES, # New Zealand\n",
    "    PT_EQUITIES, # Portugal\n",
    "    SE_EQUITIES, # Sweden\n",
    "    SG_EQUITIES, # Singapore\n",
    "    US_EQUITIES, # United States\n",
    ")\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_factor(factor, \n",
    "                    domain, \n",
    "                    start_date, \n",
    "                    end_date,\n",
    "                    factor_screen=None,\n",
    "                    quantiles=5,\n",
    "                    returns_lengths=(1, 5, 10)):\n",
    "    \"\"\"Analyze a Pipeline Factor using Alphalens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    factor : quantopian.pipeline.factors.Factor\n",
    "        Factor producing scores to be evaluated.\n",
    "    domain : quantopian.pipeline.domain.Domain\n",
    "        Domain on which the factor should be evaluated.\n",
    "    start_date : str or pd.Timestamp\n",
    "        Start date for evaluation period.\n",
    "    end_date : str or pd.Timestamp\n",
    "        End date for evaluation period.\n",
    "    standardize : \n",
    "    factor_screen : quantopian.pipeline.filters.Filter, optional\n",
    "        Filter defining which assets ``factor`` should be evaluated on.\n",
    "        Default is ``factor.notnull()``.\n",
    "    quantiles : int, optional\n",
    "        Number of buckets to use for quantile groups. Default is 5\n",
    "    returns_lengths : sequence[int]\n",
    "        Forward-returns horizons to use when evaluating ``factor``. \n",
    "        Default is 1-day, 5-day, and 10-day returns.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    factor_data : pd.DataFrame\n",
    "        A (date, asset)-indexed DataFrame with the following columns:\n",
    "            'factor': float64\n",
    "                Values produced by ``factor``.\n",
    "            'factor_quantiles': int64\n",
    "                Daily quantile label for each\n",
    "    \"\"\"\n",
    "    calendar = domain.calendar\n",
    "    # Roll input dates to the next trading session.\n",
    "    start_date = calendar.minute_to_session_label(pd.Timestamp(start_date, tz='UTC'))\n",
    "    end_date = calendar.minute_to_session_label(pd.Timestamp(end_date, tz='UTC'))\n",
    "    \n",
    "    if factor_screen is None:\n",
    "        factor_screen = factor.notnull()\n",
    "        \n",
    "    # Run pipeline to get factor values and quantiles.\n",
    "    factor_pipe = Pipeline(\n",
    "        {'factor': factor, \n",
    "         'factor_quantile': factor.quantiles(quantiles, mask=factor_screen)},\n",
    "        screen=factor_screen,\n",
    "        domain=domain,\n",
    "    )\n",
    "    factor_results = run_pipeline(factor_pipe, start_date, end_date, chunksize=250)\n",
    "    \n",
    "    column_order = []\n",
    "    returns_cols = {}\n",
    "    for length in returns_lengths:\n",
    "        colname = '{}D'.format(length)\n",
    "        column_order.append(colname)\n",
    "        # Add 1 because \"1-day\" returns needs 2 price observations.\n",
    "        returns_cols[colname] = Returns(window_length=length + 1)\n",
    "    returns_pipe = Pipeline(returns_cols, domain=domain)\n",
    "    \n",
    "    # Compute returns for the period after the factor pipeline, then \n",
    "    # shift the results back to align with our factor values.\n",
    "    returns_start_date = start_date\n",
    "    returns_end_date = end_date + domain.calendar.day * max(returns_lengths)\n",
    "    raw_returns = run_pipeline(returns_pipe, returns_start_date, returns_end_date, chunksize=500)\n",
    "    \n",
    "    shifted_returns = {}\n",
    "    for name, length in zip(column_order, returns_lengths):\n",
    "        # Shift 1-day returns back by a day, 5-day returns back by 5 days, etc.\n",
    "        raw = raw_returns[name]\n",
    "        shifted_returns[name] = backshift_returns_series(raw, length)\n",
    "        \n",
    "    # Merge backshifted returns into a single frame indexed like our desired output.\n",
    "    merged_returns = pd.DataFrame(\n",
    "        data=shifted_returns, \n",
    "        index=factor_results.index, \n",
    "        columns=column_order,\n",
    "    )\n",
    "    \n",
    "    # Concat factor results and forward returns column-wise.\n",
    "    merged = pd.concat([factor_results, merged_returns], axis=1)\n",
    "    merged.index.set_names(['date', 'asset'], inplace=True)\n",
    "    \n",
    "    # Drop NaNs\n",
    "    merged = merged.dropna(how='any')\n",
    "    \n",
    "    # Add a Business Day Offset to the DateTimeIndex\n",
    "    merged.index.levels[0].freq = pd.tseries.offsets.BDay()\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def backshift_returns_series(series, N):\n",
    "    \"\"\"Shift a multi-indexed series backwards by N observations in the first level.\n",
    "    \n",
    "    This can be used to convert backward-looking returns into a forward-returns series.\n",
    "    \"\"\"\n",
    "    ix = series.index\n",
    "    dates, sids = ix.levels\n",
    "    date_labels, sid_labels = map(np.array, ix.labels)\n",
    "    # Output date labels will contain the all but the last N dates.\n",
    "    new_dates = dates[:-N]\n",
    "    # Output data will remove the first M rows, where M is the index of the\n",
    "    # last record with one of the first N dates.\n",
    "    cutoff = date_labels.searchsorted(N)\n",
    "    new_date_labels = date_labels[cutoff:] - N\n",
    "    new_sid_labels = sid_labels[cutoff:]\n",
    "    new_values = series.values[cutoff:]\n",
    "    assert new_date_labels[0] == 0\n",
    "    new_index = pd.MultiIndex(\n",
    "        levels=[new_dates, sids],\n",
    "        labels=[new_date_labels, new_sid_labels],\n",
    "        sortorder=1,\n",
    "        names=ix.names,\n",
    "    )\n",
    "    return pd.Series(data=new_values, index=new_index)\n",
    "\n",
    "def backshift_returns_series(series, N):\n",
    "    \"\"\"Shift a multi-indexed series backwards by N observations in the first level.\n",
    "    \n",
    "    This can be used to convert backward-looking returns into a forward-returns series.\n",
    "    \"\"\"\n",
    "    ix = series.index\n",
    "    dates, sids = ix.levels\n",
    "    date_labels, sid_labels = map(np.array, ix.labels)\n",
    "\n",
    "    # Output date labels will contain the all but the last N dates.\n",
    "    new_dates = dates[:-N]\n",
    "\n",
    "    # Output data will remove the first M rows, where M is the index of the\n",
    "    # last record with one of the first N dates.\n",
    "    cutoff = date_labels.searchsorted(N)\n",
    "    new_date_labels = date_labels[cutoff:] - N\n",
    "    new_sid_labels = sid_labels[cutoff:]\n",
    "    new_values = series.values[cutoff:]\n",
    "\n",
    "    assert new_date_labels[0] == 0\n",
    "\n",
    "    new_index = pd.MultiIndex(\n",
    "        levels=[new_dates, sids],\n",
    "        labels=[new_date_labels, new_sid_labels],\n",
    "        sortorder=1,\n",
    "        names=ix.names,\n",
    "    )\n",
    "\n",
    "    return pd.Series(data=new_values, index=new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Daily5Reversal(CustomFactor):\n",
    "    \n",
    "    inputs = [EquityPricing.close]\n",
    "    window_length = 5\n",
    " \n",
    "    def compute(self,today,assets,out,close):\n",
    "        out[:] = -(close[self.window_length-1] - close[0])/close[0]\n",
    "        \n",
    "int_reversal_factor5 = Daily5Reversal()\n",
    "\n",
    "int_reversal_z = int_reversal_factor5.zscore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumPeriodReturns(CustomFactor):\n",
    "    \n",
    "    inputs = [EquityPricing.close]\n",
    "    window_length = 250 \n",
    " \n",
    "    def compute(self,today,assets,out,close):\n",
    "        out[:] = (close[self.window_length-1] - close[0])/close[0]\n",
    "        \n",
    "int_momentum_max = MomentumPeriodReturns()\n",
    "\n",
    "int_momentum_filter = (int_momentum_max.percentile_between(15,90,)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxVolume(CustomFactor):\n",
    "    inputs=[EquityPricing.volume]\n",
    "    window_length=5\n",
    "\n",
    "    def compute(self, today, asset_ids, out, values):\n",
    "        out[:] = np.min(values, axis=0)\n",
    "        \n",
    "# Create a volume filter that filters for stocks in the bottom 50% of our MinVolume factor.\n",
    "int_volume_max = MaxVolume()\n",
    "int_volume_filter = (int_volume_max.percentile_between(50, 75,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_data = evaluate_factor(\n",
    "    int_reversal_factor5, \n",
    "   Ch_EQUITIES, \n",
    "    '2010-01-01', \n",
    "    '2016-01-01',\n",
    "    quantiles=10, \n",
    "    returns_lengths=[5],\n",
    "    factor_screen=  int_volume_filter & int_momentum_filter & int_reversal_factor5.notnull() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_full_tear_sheet(al_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
